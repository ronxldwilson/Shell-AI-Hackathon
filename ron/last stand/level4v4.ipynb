{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24f60df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TabM imported successfully\n",
      "Loading data...\n",
      "🚀 Starting Level 1 training...\n",
      "📊 Level 1 models: XGB, TabM, TabPFN, LGBM, RandomForest, ExtraTrees\n",
      "Using device for TabM: cuda\n",
      "🎯 Training Level 1 for target BlendProperty1...\n",
      "  Fold 1/5\n",
      "    TabM completed successfully for fold 1\n",
      "  Fold 2/5\n",
      "    TabM completed successfully for fold 2\n",
      "  Fold 3/5\n",
      "    TabM completed successfully for fold 3\n",
      "  Fold 4/5\n",
      "    TabM completed successfully for fold 4\n",
      "  Fold 5/5\n",
      "    TabM completed successfully for fold 5\n",
      "🎯 Training Level 1 for target BlendProperty2...\n",
      "  Fold 1/5\n",
      "    TabM completed successfully for fold 1\n",
      "  Fold 2/5\n",
      "    TabM completed successfully for fold 2\n",
      "  Fold 3/5\n",
      "    TabM completed successfully for fold 3\n",
      "  Fold 4/5\n",
      "    TabM completed successfully for fold 4\n",
      "  Fold 5/5\n",
      "    TabM completed successfully for fold 5\n",
      "🎯 Training Level 1 for target BlendProperty3...\n",
      "  Fold 1/5\n",
      "    TabM completed successfully for fold 1\n",
      "  Fold 2/5\n",
      "    TabM completed successfully for fold 2\n",
      "  Fold 3/5\n",
      "    TabM completed successfully for fold 3\n",
      "  Fold 4/5\n",
      "    TabM completed successfully for fold 4\n",
      "  Fold 5/5\n",
      "    TabM completed successfully for fold 5\n",
      "🎯 Training Level 1 for target BlendProperty4...\n",
      "  Fold 1/5\n",
      "    TabM completed successfully for fold 1\n",
      "  Fold 2/5\n",
      "    TabM completed successfully for fold 2\n",
      "  Fold 3/5\n",
      "    TabM completed successfully for fold 3\n",
      "  Fold 4/5\n",
      "    TabM completed successfully for fold 4\n",
      "  Fold 5/5\n",
      "    TabM completed successfully for fold 5\n",
      "🎯 Training Level 1 for target BlendProperty5...\n",
      "  Fold 1/5\n",
      "    TabM completed successfully for fold 1\n",
      "  Fold 2/5\n",
      "    TabM completed successfully for fold 2\n",
      "  Fold 3/5\n",
      "    TabM completed successfully for fold 3\n",
      "  Fold 4/5\n",
      "    TabM completed successfully for fold 4\n",
      "  Fold 5/5\n",
      "    TabM completed successfully for fold 5\n",
      "🎯 Training Level 1 for target BlendProperty6...\n",
      "  Fold 1/5\n",
      "    TabM completed successfully for fold 1\n",
      "  Fold 2/5\n",
      "    TabM completed successfully for fold 2\n",
      "  Fold 3/5\n",
      "    TabM completed successfully for fold 3\n",
      "  Fold 4/5\n",
      "    TabM completed successfully for fold 4\n",
      "  Fold 5/5\n",
      "    TabM completed successfully for fold 5\n",
      "🎯 Training Level 1 for target BlendProperty7...\n",
      "  Fold 1/5\n",
      "    TabM completed successfully for fold 1\n",
      "  Fold 2/5\n",
      "    TabM completed successfully for fold 2\n",
      "  Fold 3/5\n",
      "    TabM completed successfully for fold 3\n",
      "  Fold 4/5\n",
      "    TabM completed successfully for fold 4\n",
      "  Fold 5/5\n",
      "    TabM completed successfully for fold 5\n",
      "🎯 Training Level 1 for target BlendProperty8...\n",
      "  Fold 1/5\n",
      "    TabM completed successfully for fold 1\n",
      "  Fold 2/5\n",
      "    TabM completed successfully for fold 2\n",
      "  Fold 3/5\n",
      "    TabM completed successfully for fold 3\n",
      "  Fold 4/5\n",
      "    TabM completed successfully for fold 4\n",
      "  Fold 5/5\n",
      "    TabM completed successfully for fold 5\n",
      "🎯 Training Level 1 for target BlendProperty9...\n",
      "  Fold 1/5\n",
      "    TabM completed successfully for fold 1\n",
      "  Fold 2/5\n",
      "    TabM completed successfully for fold 2\n",
      "  Fold 3/5\n",
      "    TabM completed successfully for fold 3\n",
      "  Fold 4/5\n",
      "    TabM completed successfully for fold 4\n",
      "  Fold 5/5\n",
      "    TabM completed successfully for fold 5\n",
      "🎯 Training Level 1 for target BlendProperty10...\n",
      "  Fold 1/5\n",
      "    TabM completed successfully for fold 1\n",
      "  Fold 2/5\n",
      "    TabM completed successfully for fold 2\n",
      "  Fold 3/5\n",
      "    TabM completed successfully for fold 3\n",
      "  Fold 4/5\n",
      "    TabM completed successfully for fold 4\n",
      "  Fold 5/5\n",
      "    TabM completed successfully for fold 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 209\u001b[0m\n\u001b[0;32m    207\u001b[0m     tabpfn\u001b[38;5;241m.\u001b[39mfit(X_tr, y_tr\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m    208\u001b[0m     level1_oof[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTabPFN\u001b[39m\u001b[38;5;124m'\u001b[39m][val_idx, t] \u001b[38;5;241m=\u001b[39m tabpfn\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m--> 209\u001b[0m     level1_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTabPFN\u001b[39m\u001b[38;5;124m'\u001b[39m][:, t] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtabpfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m kf\u001b[38;5;241m.\u001b[39mn_splits\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Error with TabPFN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\D Drive\\ron\\last stand\\.venv\\lib\\site-packages\\tabpfn\\regressor.py:796\u001b[0m, in \u001b[0;36mTabPFNRegressor.predict\u001b[1;34m(self, X, output_type, quantiles)\u001b[0m\n\u001b[0;32m    789\u001b[0m X \u001b[38;5;241m=\u001b[39m _process_text_na_dataframe(X, ord_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor_)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;66;03m# Runs over iteration engine\u001b[39;00m\n\u001b[0;32m    792\u001b[0m (\n\u001b[0;32m    793\u001b[0m     _,\n\u001b[0;32m    794\u001b[0m     outputs,  \u001b[38;5;66;03m# list of tensors [N_est, N_samples, N_borders] (after forward)\u001b[39;00m\n\u001b[0;32m    795\u001b[0m     borders,  \u001b[38;5;66;03m# list of numpy arrays containing borders for each estimator\u001b[39;00m\n\u001b[1;32m--> 796\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_inference_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;66;03m# --- Translate probs, average, get final logits ---\u001b[39;00m\n\u001b[0;32m    799\u001b[0m transformed_logits \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    800\u001b[0m     translate_probs_across_borders(\n\u001b[0;32m    801\u001b[0m         logits,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m logits, borders_t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, borders)\n\u001b[0;32m    806\u001b[0m ]\n",
      "File \u001b[1;32mc:\\D Drive\\ron\\last stand\\.venv\\lib\\site-packages\\tabpfn\\regressor.py:924\u001b[0m, in \u001b[0;36mTabPFNRegressor.forward\u001b[1;34m(self, X, use_inference_mode)\u001b[0m\n\u001b[0;32m    921\u001b[0m borders: \u001b[38;5;28mlist\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Iterate over estimators\u001b[39;00m\n\u001b[1;32m--> 924\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor_\u001b[38;5;241m.\u001b[39miter_outputs(\n\u001b[0;32m    925\u001b[0m     X,\n\u001b[0;32m    926\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_,\n\u001b[0;32m    927\u001b[0m     autocast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_autocast_,\n\u001b[0;32m    928\u001b[0m ):\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax_temperature \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    930\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax_temperature  \u001b[38;5;66;03m# noqa: PLW2901\u001b[39;00m\n",
      "File \u001b[1;32mc:\\D Drive\\ron\\last stand\\.venv\\lib\\site-packages\\tabpfn\\inference.py:474\u001b[0m, in \u001b[0;36mInferenceEngineCachePreprocessing.iter_outputs\u001b[1;34m(self, X, device, autocast, only_return_standard_out)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X_train, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    473\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(X_train, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# noqa: PLW2901\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: PLW2901\u001b[39;00m\n\u001b[0;32m    475\u001b[0m X_test \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mtransform(X)\u001b[38;5;241m.\u001b[39mX \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_preprocessing \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X_test, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, BayesianRidge, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Try to import TabPFN and TabM with fallbacks\n",
    "try:\n",
    "    from tabpfn import TabPFNRegressor\n",
    "    TABPFN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️  TabPFN not available. Install with: pip install tabpfn\")\n",
    "    TABPFN_AVAILABLE = False\n",
    "\n",
    "# TabM proper import - check if it's available and get the correct class\n",
    "try:\n",
    "    from tabm import TabM\n",
    "    TABM_AVAILABLE = True\n",
    "    print(\"✅ TabM imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  TabM not available. Install from GitHub: pip install git+https://github.com/yandex-research/tabm.git\")\n",
    "    TABM_AVAILABLE = False\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "X = train.drop([f'BlendProperty{i}' for i in range(1, 11)], axis=1)\n",
    "y = train[[f'BlendProperty{i}' for i in range(1, 11)]]\n",
    "X_test = test.drop(['ID'], axis=1)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"🚀 Starting Level 1 training...\")\n",
    "\n",
    "# Level 1: Advanced models as base models - ALWAYS include XGBoost\n",
    "level1_names = ['XGB']  # XGBoost is always included in Level 1\n",
    "\n",
    "# Add available advanced models to level 1\n",
    "if TABM_AVAILABLE:\n",
    "    level1_names.append('TabM')\n",
    "if TABPFN_AVAILABLE:\n",
    "    level1_names.append('TabPFN')\n",
    "\n",
    "# Add additional strong models to Level 1 for better diversity\n",
    "level1_names.extend(['LGBM', 'RandomForest', 'ExtraTrees'])\n",
    "\n",
    "print(f\"📊 Level 1 models: {', '.join(level1_names)}\")\n",
    "\n",
    "level1_oof = {name: np.zeros(y.shape) for name in level1_names}\n",
    "level1_test = {name: np.zeros((X_test.shape[0], y.shape[1])) for name in level1_names}\n",
    "\n",
    "# Device for TabM\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device for TabM: {device}\")\n",
    "\n",
    "for t in range(y.shape[1]):\n",
    "    print(f\"🎯 Training Level 1 for target BlendProperty{t+1}...\")\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\"  Fold {fold+1}/5\")\n",
    "        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[tr_idx, t], y.iloc[val_idx, t]\n",
    "\n",
    "        # XGBoost - ALWAYS included with optimized parameters\n",
    "        xgb_model = xgb.XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            verbosity=0,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        xgb_model.fit(X_tr, y_tr)\n",
    "        level1_oof['XGB'][val_idx, t] = xgb_model.predict(X_val)\n",
    "        level1_test['XGB'][:, t] += xgb_model.predict(X_test) / kf.n_splits\n",
    "\n",
    "        # LightGBM with optimized parameters\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        lgb_model.fit(X_tr, y_tr)\n",
    "        level1_oof['LGBM'][val_idx, t] = lgb_model.predict(X_val)\n",
    "        level1_test['LGBM'][:, t] += lgb_model.predict(X_test) / kf.n_splits\n",
    "\n",
    "        # RandomForest with increased estimators\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_tr, y_tr)\n",
    "        level1_oof['RandomForest'][val_idx, t] = rf.predict(X_val)\n",
    "        level1_test['RandomForest'][:, t] += rf.predict(X_test) / kf.n_splits\n",
    "\n",
    "        # ExtraTrees with increased estimators\n",
    "        et = ExtraTreesRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        et.fit(X_tr, y_tr)\n",
    "        level1_oof['ExtraTrees'][val_idx, t] = et.predict(X_val)\n",
    "        level1_test['ExtraTrees'][:, t] += et.predict(X_test) / kf.n_splits\n",
    "\n",
    "        # TabM (if available) - Fixed implementation\n",
    "        if 'TabM' in level1_names and TABM_AVAILABLE:\n",
    "            try:\n",
    "                # Prepare data for TabM\n",
    "                scaler_X = StandardScaler()\n",
    "                scaler_y = StandardScaler()\n",
    "                \n",
    "                X_tr_scaled = scaler_X.fit_transform(X_tr.values)\n",
    "                y_tr_scaled = scaler_y.fit_transform(y_tr.values.reshape(-1, 1)).flatten()\n",
    "                X_val_scaled = scaler_X.transform(X_val.values)\n",
    "                X_test_scaled = scaler_X.transform(X_test.values)\n",
    "                \n",
    "                # Create TabM model with proper parameters\n",
    "                tabm_model = TabM.make(\n",
    "                    n_num_features=X_tr_scaled.shape[1],\n",
    "                    cat_cardinalities=None,\n",
    "                    d_out=1,  # Single target for regression\n",
    "                    k=32  # Ensemble size\n",
    "                )\n",
    "                tabm_model.to(device)\n",
    "                \n",
    "                # Convert to tensors\n",
    "                X_tr_tensor = torch.tensor(X_tr_scaled, dtype=torch.float32).to(device)\n",
    "                y_tr_tensor = torch.tensor(y_tr_scaled, dtype=torch.float32).to(device)\n",
    "                X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "                X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "                \n",
    "                # Training TabM\n",
    "                optimizer = torch.optim.AdamW(tabm_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "                criterion = nn.MSELoss()\n",
    "                \n",
    "                # Simple training loop for TabM\n",
    "                tabm_model.train()\n",
    "                for epoch in range(50):  # Limited epochs for efficiency\n",
    "                    optimizer.zero_grad()\n",
    "                    preds = tabm_model(X_tr_tensor)\n",
    "                    if preds.dim() > 1 and preds.shape[1] > 1:\n",
    "                        preds = torch.median(preds, dim=1)[0]\n",
    "                    elif preds.dim() > 1:\n",
    "                        preds = preds.squeeze()\n",
    "                    loss = criterion(preds, y_tr_tensor)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Prediction\n",
    "                tabm_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_preds = tabm_model(X_val_tensor)\n",
    "                    if val_preds.dim() > 1 and val_preds.shape[1] > 1:\n",
    "                        val_preds = torch.median(val_preds, dim=1)[0]\n",
    "                    elif val_preds.dim() > 1:\n",
    "                        val_preds = val_preds.squeeze()\n",
    "                    \n",
    "                    test_preds = tabm_model(X_test_tensor)\n",
    "                    if test_preds.dim() > 1 and test_preds.shape[1] > 1:\n",
    "                        test_preds = torch.median(test_preds, dim=1)[0]\n",
    "                    elif test_preds.dim() > 1:\n",
    "                        test_preds = test_preds.squeeze()\n",
    "                \n",
    "                # Scale back predictions\n",
    "                val_preds_scaled = scaler_y.inverse_transform(val_preds.cpu().numpy().reshape(-1, 1)).flatten()\n",
    "                test_preds_scaled = scaler_y.inverse_transform(test_preds.cpu().numpy().reshape(-1, 1)).flatten()\n",
    "                \n",
    "                level1_oof['TabM'][val_idx, t] = val_preds_scaled\n",
    "                level1_test['TabM'][:, t] += test_preds_scaled / kf.n_splits\n",
    "                \n",
    "                print(f\"    TabM completed successfully for fold {fold+1}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error with TabM: {e}\")\n",
    "                # Remove TabM from level1_names if it fails\n",
    "                if 'TabM' in level1_names:\n",
    "                    level1_names.remove('TabM')\n",
    "                    print(f\"    TabM removed from ensemble due to error\")\n",
    "\n",
    "        # TabPFN (if available)\n",
    "        if 'TabPFN' in level1_names and TABPFN_AVAILABLE:\n",
    "            try:\n",
    "                tabpfn = TabPFNRegressor(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                tabpfn.fit(X_tr, y_tr.values)\n",
    "                level1_oof['TabPFN'][val_idx, t] = tabpfn.predict(X_val)\n",
    "                level1_test['TabPFN'][:, t] += tabpfn.predict(X_test) / kf.n_splits\n",
    "            except Exception as e:\n",
    "                print(f\"    Error with TabPFN: {e}\")\n",
    "                if 'TabPFN' in level1_names:\n",
    "                    level1_names.remove('TabPFN')\n",
    "\n",
    "# Remove empty entries from level1 dictionaries\n",
    "level1_names = [name for name in level1_names if name in level1_oof and level1_oof[name].sum() != 0]\n",
    "level1_oof = {name: level1_oof[name] for name in level1_names}\n",
    "level1_test = {name: level1_test[name] for name in level1_names}\n",
    "\n",
    "print(f\"\\n📊 Level 1 MAPE Scores (using {len(level1_names)} models):\")\n",
    "for name in level1_names:\n",
    "    mape = mean_absolute_percentage_error(y, level1_oof[name])\n",
    "    print(f\"  {name}: {mape:.6f}\")\n",
    "\n",
    "print(\"\\n🔄 Preparing Level 2 inputs from Level 1 outputs...\")\n",
    "stack_X = np.concatenate([level1_oof[name] for name in level1_names], axis=1)\n",
    "stack_X_test = np.concatenate([level1_test[name] for name in level1_names], axis=1)\n",
    "\n",
    "print(\"\\n🚀 Starting Level 2 stacking (Traditional ML Models)...\")\n",
    "\n",
    "# Level 2: Traditional ML models as stacking models\n",
    "level2_names = [\n",
    "    'Ridge', 'Lasso', 'ElasticNet', 'BayesianRidge', 'Huber',\n",
    "    'RandomForest_L2', 'ExtraTrees_L2', 'AdaBoost', 'GradientBoost', 'Bagging',\n",
    "    'KNN', 'SVR', 'XGB_L2', 'LGBM_L2'  # Added L2 suffix to avoid confusion\n",
    "]\n",
    "level2_oof = {name: np.zeros(y.shape) for name in level2_names}\n",
    "level2_test = {name: np.zeros((X_test.shape[0], y.shape[1])) for name in level2_names}\n",
    "\n",
    "for t in range(y.shape[1]):\n",
    "    print(f\"🎯 Level 2 stacking for BlendProperty{t+1}...\")\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(stack_X)):\n",
    "        print(f\"  Fold {fold+1}/5\")\n",
    "        X_tr, X_val = stack_X[tr_idx], stack_X[val_idx]\n",
    "        y_tr, y_val = y.iloc[tr_idx, t], y.iloc[val_idx, t]\n",
    "\n",
    "        # Ridge\n",
    "        ridge = Ridge(alpha=1.0)\n",
    "        ridge.fit(X_tr, y_tr)\n",
    "        level2_oof['Ridge'][val_idx, t] = ridge.predict(X_val)\n",
    "        level2_test['Ridge'][:, t] += ridge.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "        # Lasso\n",
    "        lasso = Lasso(alpha=0.1, max_iter=1000)\n",
    "        lasso.fit(X_tr, y_tr)\n",
    "        level2_oof['Lasso'][val_idx, t] = lasso.predict(X_val)\n",
    "        level2_test['Lasso'][:, t] += lasso.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "        # ElasticNet\n",
    "        elastic = ElasticNet(alpha=0.1, max_iter=1000)\n",
    "        elastic.fit(X_tr, y_tr)\n",
    "        level2_oof['ElasticNet'][val_idx, t] = elastic.predict(X_val)\n",
    "        level2_test['ElasticNet'][:, t] += elastic.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "        # BayesianRidge\n",
    "        bayesian = BayesianRidge()\n",
    "        bayesian.fit(X_tr, y_tr)\n",
    "        level2_oof['BayesianRidge'][val_idx, t] = bayesian.predict(X_val)\n",
    "        level2_test['BayesianRidge'][:, t] += bayesian.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "        # Huber\n",
    "        huber = HuberRegressor()\n",
    "        huber.fit(X_tr, y_tr)\n",
    "        level2_oof['Huber'][val_idx, t] = huber.predict(X_val)\n",
    "        level2_test['Huber'][:, t] += huber.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "        # RandomForest (Level 2)\n",
    "        rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        rf.fit(X_tr, y_tr)\n",
    "        level2_oof['RandomForest_L2'][val_idx, t] = rf.predict(X_val)\n",
    "        level2_test['RandomForest_L2'][:, t] += rf.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "        # ExtraTrees (Level 2)\n",
    "        et = ExtraTreesRegressor(n_estimators=50, random_state=42)\n",
    "        et.fit(X_tr, y_tr)\n",
    "        level2_oof['ExtraTrees_L2'][val_idx, t] = et.predict(X_val)\n",
    "        level2_test['ExtraTrees_L2'][:, t] += et.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "        # AdaBoost\n",
    "        ada = AdaBoostRegressor(random_state=42, n_estimators=50)\n",
    "        ada.fit(X_tr, y_tr)\n",
    "        level2_oof['AdaBoost'][val_idx, t] = ada.predict(X_val)\n",
    "        level2_test['AdaBoost'][:, t] += ada.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "        # GradientBoosting\n",
    "        gb = GradientBoostingRegressor(random_state=42, n_estimators=100)\n",
    "        gb.fit(X_tr, y_tr)\n",
    "        level2_oof['GradientBoost'][val_idx, t] = gb.predict(X_val)\n",
    "        level2_test['GradientBoost'][:, t] += gb.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "        # Bagging\n",
    "        bag = BaggingRegressor(random_state=42, n_estimators=50)\n",
    "        bag.fit(X_tr, y_tr)\n",
    "        level2_oof['Bagging'][val_idx, t] = bag.predict(X_val)\n",
    "        level2_test['Bagging'][:, t] += bag.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "        # KNN\n",
    "        knn = KNeighborsRegressor(n_neighbors=5)\n",
    "        knn.fit(X_tr, y_tr)\n",
    "        level2_oof['KNN'][val_idx, t] = knn.predict(X_val)\n",
    "        level2_test['KNN'][:, t] += knn.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "        # SVR\n",
    "        svr = SVR(kernel='rbf', gamma='scale')\n",
    "        svr.fit(X_tr, y_tr)\n",
    "        level2_oof['SVR'][val_idx, t] = svr.predict(X_val)\n",
    "        level2_test['SVR'][:, t] += svr.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "        # XGBoost (Level 2)\n",
    "        xgb_model = xgb.XGBRegressor(random_state=42, verbosity=0)\n",
    "        xgb_model.fit(X_tr, y_tr)\n",
    "        level2_oof['XGB_L2'][val_idx, t] = xgb_model.predict(X_val)\n",
    "        level2_test['XGB_L2'][:, t] += xgb_model.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "        # LightGBM (Level 2)\n",
    "        lgb_model = lgb.LGBMRegressor(random_state=42, verbose=-1)\n",
    "        lgb_model.fit(X_tr, y_tr)\n",
    "        level2_oof['LGBM_L2'][val_idx, t] = lgb_model.predict(X_val)\n",
    "        level2_test['LGBM_L2'][:, t] += lgb_model.predict(stack_X_test) / kf.n_splits\n",
    "\n",
    "print(\"\\n📊 Level 2 MAPE Scores:\")\n",
    "level2_scores = {}\n",
    "for name in level2_names:\n",
    "    mape = mean_absolute_percentage_error(y, level2_oof[name])\n",
    "    level2_scores[name] = mape\n",
    "    print(f\"  {name}: {mape:.6f}\")\n",
    "\n",
    "print(\"\\n🔍 Starting Dynamic Level 3 optimization...\")\n",
    "\n",
    "# Dynamic selection of top performers (configurable top K models)\n",
    "TOP_K = min(6, len(level2_names))  # Select top 6 models or all if less than 6\n",
    "sorted_models = sorted(level2_scores.items(), key=lambda x: x[1])\n",
    "top_models = [model[0] for model in sorted_models[:TOP_K]]\n",
    "\n",
    "print(f\"\\n🎯 Selected top {len(top_models)} models for Level 3:\")\n",
    "for i, model in enumerate(top_models):\n",
    "    print(f\"  {i+1}. {model}: {level2_scores[model]:.6f}\")\n",
    "\n",
    "# Prepare Level 3 inputs with top performers\n",
    "level3_oof = np.concatenate([level2_oof[model] for model in top_models], axis=1)\n",
    "level3_test = np.concatenate([level2_test[model] for model in top_models], axis=1)\n",
    "\n",
    "def objective(trial):\n",
    "    # Dynamically suggest weights for each top model\n",
    "    weights = {}\n",
    "    for model in top_models:\n",
    "        weights[model] = trial.suggest_float(f'w_{model}', 0.0, 1.0)\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_weight = sum(weights.values())\n",
    "    if total_weight == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    normalized_weights = {k: v/total_weight for k, v in weights.items()}\n",
    "    \n",
    "    # Create weighted ensemble\n",
    "    ensemble_pred = sum(normalized_weights[model] * level2_oof[model] \n",
    "                       for model in top_models)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y, ensemble_pred)\n",
    "    return mape\n",
    "\n",
    "# Optimize weights\n",
    "print(\"🔧 Optimizing ensemble weights...\")\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=200, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n✅ Best MAPE: {study.best_value:.6f}\")\n",
    "print(\"🎯 Best weights:\")\n",
    "best_params = study.best_params\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value:.4f}\")\n",
    "\n",
    "# Normalize best weights\n",
    "total_weight = sum(best_params.values())\n",
    "normalized_weights = {k: v/total_weight for k, v in best_params.items()}\n",
    "\n",
    "print(\"\\n📊 Normalized weights:\")\n",
    "for param, value in normalized_weights.items():\n",
    "    print(f\"  {param}: {value:.4f}\")\n",
    "\n",
    "# Create final ensemble predictions\n",
    "final_test = sum(normalized_weights[f'w_{model}'] * level2_test[model] \n",
    "                 for model in top_models)\n",
    "\n",
    "# Final validation score\n",
    "final_oof = sum(normalized_weights[f'w_{model}'] * level2_oof[model] \n",
    "                for model in top_models)\n",
    "\n",
    "final_mape = mean_absolute_percentage_error(y, final_oof)\n",
    "print(f\"\\n🎉 Final ensemble MAPE: {final_mape:.6f}\")\n",
    "\n",
    "print(\"\\n📊 Individual target MAPE scores:\")\n",
    "for i in range(y.shape[1]):\n",
    "    target_mape = mean_absolute_percentage_error(y.iloc[:, i], final_oof[:, i])\n",
    "    print(f\"  BlendProperty{i+1}: {target_mape:.6f}\")\n",
    "\n",
    "# Save submission with descriptive filename\n",
    "submission = pd.DataFrame(final_test, columns=[f'BlendProperty{i}' for i in range(1, 11)])\n",
    "submission.insert(0, 'ID', test['ID'])\n",
    "submission_filename = f\"submission_xgb_level1_ensemble_top{TOP_K}.csv\"\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "print(f\"\\n💾 Submission file saved as '{submission_filename}'\")\n",
    "\n",
    "print(f\"\\n🎯 Advanced Dynamic Ensemble Summary:\")\n",
    "print(f\"  Level 1: {len(level1_names)} strong models including XGBoost: {', '.join(level1_names)}\")\n",
    "print(f\"  Level 2: {len(level2_names)} traditional ML models stacking on Level 1 outputs\") \n",
    "print(f\"  Level 3: Dynamic optimization with top {len(top_models)} performers: {', '.join(top_models)}\")\n",
    "print(f\"  Final MAPE: {final_mape:.6f}\")\n",
    "print(f\"  Submission saved as: {submission_filename}\")\n",
    "\n",
    "# Instructions for installing missing packages\n",
    "print(\"\\n📝 Installation Instructions:\")\n",
    "if not TABM_AVAILABLE:\n",
    "    print(\"  For TabM: pip install git+https://github.com/yandex-research/tabm.git\")\n",
    "if not TABPFN_AVAILABLE:\n",
    "    print(\"  For TabPFN: pip install tabpfn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
